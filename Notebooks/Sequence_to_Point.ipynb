{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import (\n",
    "    RBFKernel,\n",
    "    ScaleKernel,\n",
    "    PeriodicKernel,\n",
    "    MaternKernel,\n",
    "    CosineKernel,\n",
    "    LinearKernel,\n",
    ")\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "dist = tfp.distributions\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utilities import plot, errors\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latexify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "except ModuleNotFoundError:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "\n",
    "os.environ[\"LATEXIFY\"] = \"1\"\n",
    "os.environ[\"FIG_DIR\"] = \"./Figures/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_load(appliances, train, test=None, linear=False, bias=False):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    train_range = []\n",
    "    x_train_timestamp = []\n",
    "    n = 99\n",
    "    units_to_pad = n // 2\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_range = StandardScaler()\n",
    "\n",
    "    # train\n",
    "    for key, values in train.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"Data/Building{key}_NILM_data_basic.csv\",\n",
    "                usecols=[\"Timestamp\", \"main\", appliances[app]],\n",
    "            )\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "\n",
    "        timestamp_train = (\n",
    "            pd.to_datetime(df[\"Timestamp\"]).astype(int) / 10**18\n",
    "        ).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad), \"constant\", constant_values=(0, 0))\n",
    "        x = jnp.array([x[i : i + n] for i in range(len(x) - n + 1)])\n",
    "        x_train.extend(x)\n",
    "        y_train.extend(y)\n",
    "        train_range.extend(jnp.max(x, axis=1) - jnp.min(x, axis=1))\n",
    "        x_train_timestamp.extend(torch.tensor(timestamp_train))\n",
    "\n",
    "    x_train = jnp.array(x_train)\n",
    "    y_train = jnp.array(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(-1, 1)\n",
    "    x_train_range = jnp.array(train_range).reshape(-1, 1)\n",
    "\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "    x_train_range = scaler_range.fit_transform(x_train_range)\n",
    "\n",
    "    # test\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    test_range = []\n",
    "    x_test_timestamp = []\n",
    "    x_test_timestamp_true = []\n",
    "    for key, values in test.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"Data/Building{key}_NILM_data_basic.csv\",\n",
    "                usecols=[\"Timestamp\", \"main\", appliances[app]],\n",
    "            )\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "\n",
    "        if bias == True:\n",
    "            x = x + 100 * np.ones(x.shape[0])\n",
    "        timestamp_true = df[\"Timestamp\"].values\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int) / 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad), \"constant\", constant_values=(0, 0))\n",
    "        x = jnp.array([x[i : i + n] for i in range(len(x) - n + 1)])\n",
    "        x_test.extend(x)\n",
    "        y_test.extend(y)\n",
    "        test_range.append(jnp.max(x, axis=1) - jnp.min(x, axis=1))\n",
    "        x_test_timestamp_true.extend(timestamp_true)\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "\n",
    "    x_test = jnp.array(x_test)\n",
    "    y_test = jnp.array(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(-1, 1)\n",
    "    x_test_range = jnp.array(test_range).reshape(-1, 1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "    x_test_range = scaler_range.transform(x_test_range)\n",
    "\n",
    "    x_train = jnp.array(x_train).reshape(x_train.shape[0], n)\n",
    "    y_train = jnp.array(y_train)\n",
    "    x_train_range = jnp.array(x_train_range)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(\n",
    "        x_train_timestamp.shape[0], 1\n",
    "    )\n",
    "    x_test = jnp.array(x_test).reshape(x_test.shape[0], n)\n",
    "    y_test = jnp.array(y_test)\n",
    "    x_test_timestamp = (\n",
    "        torch.tensor(x_test_timestamp)\n",
    "        .reshape(x_test_timestamp.shape[0], 1)\n",
    "        .to(torch.float64)\n",
    "    )\n",
    "    x_test_range = jnp.array(x_test_range).reshape(-1, 1)\n",
    "\n",
    "    if linear == True:\n",
    "        n = 100\n",
    "        x_train = jnp.concatenate((x_train, x_train_range), axis=1).reshape(\n",
    "            x_train.shape[0], n\n",
    "        )\n",
    "        x_test = jnp.concatenate((x_test, x_test_range), axis=1).reshape(\n",
    "            x_test.shape[0], n\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        x_train_timestamp,\n",
    "        x_test_timestamp,\n",
    "        x_test_timestamp_true,\n",
    "        scaler_x,\n",
    "        scaler_y,\n",
    "        scaler_time,\n",
    "        scaler_range,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {\n",
    "    1: {\"start_time\": \"2011-04-28\", \"end_time\": \"2011-05-15\"},\n",
    "    3: {\"start_time\": \"2011-04-19\", \"end_time\": \"2011-05-22\"},\n",
    "}\n",
    "test = {\n",
    "    2: {\"start_time\": \"2011-04-21\", \"end_time\": \"2011-05-21\"},\n",
    "}\n",
    "appliances = [\"Microwave\", \"Refrigerator\", \"Dish Washer\"]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = False\n",
    "bias = False\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    x_train_timestamp,\n",
    "    x_test_timestamp,\n",
    "    x_test_time_true,\n",
    "    scaler_x,\n",
    "    scaler_y,\n",
    "    scaler_time,\n",
    "    scaler_range,\n",
    ") = dataset_load(appliances, train, test, linear=linear, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.array(x_train)).to(torch.float32)\n",
    "y = (\n",
    "    torch.tensor(np.array(y_train))\n",
    "    .reshape(\n",
    "        -1,\n",
    "    )\n",
    "    .to(torch.float32)\n",
    ")\n",
    "xt = torch.tensor(np.array(x_test)).to(torch.float32)\n",
    "yt = (\n",
    "    torch.tensor(np.array(y_test))\n",
    "    .reshape(\n",
    "        -1,\n",
    "    )\n",
    "    .to(torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_model(train, test, linear, ard, model_name):\n",
    "    kernel1 = ScaleKernel(MaternKernel(nu=2.5, ard_num_dims=ard))\n",
    "    kernel = kernel1\n",
    "    if linear:\n",
    "        kernel2 = ScaleKernel(LinearKernel(active_dims=(99)))\n",
    "        kernel = kernel1 + kernel2\n",
    "    inducing_points = x[np.arange(0, x.shape[0], 20)]\n",
    "\n",
    "    model = SGPRegressor(x.to(\"cuda\"), y.to(\"cuda\"), kernel, inducing_points).to(\"cuda\")\n",
    "    if train:\n",
    "        loss = model.fit(lr=1e-2, n_epochs=1500, verbose=1, random_state=0)\n",
    "\n",
    "        plt.plot(np.asarray(loss[0]))\n",
    "\n",
    "        model_name = model_name\n",
    "        torch.save(model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "    if test:\n",
    "        model_name = model_name\n",
    "        model.load_state_dict(torch.load(os.path.join(\"./models\", model_name)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if linear:\n",
    "    model_name = \"Seq_to_pt_linear_final.pt\"\n",
    "else:\n",
    "    model_name = \"Seq_to_pt_final.pt\"\n",
    "\n",
    "model = GP_model(\n",
    "    train=False, test=True, linear=linear, ard=x.shape[1], model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = model.predict((xt).to(\"cuda\"))\n",
    "y_mean = pred_dist.loc\n",
    "y_mean = scaler_y.inverse_transform(y_mean.reshape(-1, 1).cpu()).squeeze()\n",
    "\n",
    "print(y_test.shape, y_mean.shape)\n",
    "y_mean = np.clip(y_mean, 0, y_mean.max(), out=y_mean)\n",
    "var_pred = pred_dist.variance\n",
    "var_pred = scaler_y.inverse_transform(var_pred.reshape(-1, 1).detach().cpu()).squeeze()\n",
    "std_pred = pred_dist.stddev\n",
    "std_pred = torch.tensor(\n",
    "    scaler_y.inverse_transform(std_pred.reshape(-1, 1).detach().cpu()).squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = errors.mae(torch.tensor(y_mean), yt)\n",
    "msll = errors.msll(var_pred, y_mean, yt)\n",
    "qce = errors.qce(std_pred, y_mean, yt)\n",
    "print(\"mae, msll, qce - \", mae, msll, qce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [4000, 4800, 13000]\n",
    "idx = [500, 200, 300]\n",
    "\n",
    "if bias:\n",
    "    start = [4170]\n",
    "    idx = [300]\n",
    "\n",
    "x = scaler_x.inverse_transform(xt[:, 0:99])[:, 49]\n",
    "i = 0\n",
    "for i in range(len(start)):\n",
    "    if bias:\n",
    "        plot.prediction_plots(\n",
    "            x, yt, y_mean, start[i], idx[i], var_pred, \"Seq_to_point_bias\", i\n",
    "        )\n",
    "    else:\n",
    "        plot.prediction_plots(\n",
    "            x,\n",
    "            yt,\n",
    "            y_mean,\n",
    "            start[i],\n",
    "            idx[i],\n",
    "            var_pred,\n",
    "            \"Seq_to_point_plt\" + str(i + 1),\n",
    "            i,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "latexify(width_scale_factor=2.5, fig_height=1.75)\n",
    "sigma_pred = jnp.sqrt(var_pred)\n",
    "df, df1 = plot.calibration_regression(\n",
    "    y_mean.squeeze(), sigma_pred.squeeze(), y_test.squeeze(), \"test\", \"r\", ax\n",
    ")\n",
    "ax.legend()\n",
    "if bias:\n",
    "    if linear:\n",
    "        savefig(\"Sequence_to_point_linear_bias_calibration\")\n",
    "    else:\n",
    "        savefig(\"Sequence_to_point_bias_calibration\")\n",
    "elif linear:\n",
    "    savefig(\"Sequence_to_point_linear_calibration\")\n",
    "else:\n",
    "    savefig(\"Sequence_to_point_calibration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = errors.find_p_hat(np.array(yt), y_mean, sigma_pred)\n",
    "p = cal.index\n",
    "mae_cal = errors.ace(p.values, cal.values)\n",
    "print(\"calibration error: \", mae_cal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a19952a8cb0d513e360355f3718fc7b5b0ccef7313ddd97e7b7ab66b1ecfbb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
