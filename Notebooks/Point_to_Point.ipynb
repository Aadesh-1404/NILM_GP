{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import (\n",
    "    RBFKernel,\n",
    "    ScaleKernel,\n",
    "    PeriodicKernel,\n",
    "    MaternKernel,\n",
    "    CosineKernel,\n",
    ")\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor, ExactGPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "dist = tfp.distributions\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.metrics import (\n",
    "    mean_standardized_log_loss,\n",
    "    negative_log_predictive_density,\n",
    "    mean_squared_error,\n",
    ")\n",
    "import time\n",
    "import numpy as np\n",
    "from utilities import errors, plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latexifying Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "except ModuleNotFoundError:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "\n",
    "os.environ[\"LATEXIFY\"] = \"1\"\n",
    "os.environ[\"FIG_DIR\"] = \"./Figures/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_load(appliances, train, test=None, bias=False):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    train_time = []\n",
    "    x_train_timestamp = []\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    app = 0\n",
    "\n",
    "    ### train\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    for key, values in train.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"Data/Building{key}_NILM_data_basic.csv\",\n",
    "                usecols=[\"Timestamp\", \"main\", appliances[app]],\n",
    "            )\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "\n",
    "        timetrain = df[\"Timestamp\"]\n",
    "        timestamp_train = (\n",
    "            pd.to_datetime(df[\"Timestamp\"]).astype(int) / 10**18\n",
    "        ).values\n",
    "\n",
    "        x_train.extend(torch.tensor(x))\n",
    "        y_train.extend(torch.tensor(y))\n",
    "        x_train_timestamp.extend(torch.tensor(timestamp_train))\n",
    "        train_time.extend(timetrain)\n",
    "\n",
    "    x_train = torch.tensor(x_train).reshape(-1, 1)\n",
    "    y_train = torch.tensor(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(-1, 1)\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "\n",
    "    ## test\n",
    "    x_test = []\n",
    "    test_time = []\n",
    "    y_test = []\n",
    "    x_test_timestamp = []\n",
    "    app = 0\n",
    "    for key, values in test.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"Data/Building{key}_NILM_data_basic.csv\",\n",
    "                usecols=[\"Timestamp\", \"main\", appliances[app]],\n",
    "            )\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "\n",
    "        timetest = df[\"Timestamp\"]\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int) / 10**18).values\n",
    "\n",
    "        if bias == True:\n",
    "            x = x + 100 * np.ones(x.shape[0])\n",
    "        x_test.extend(torch.tensor(x))\n",
    "        y_test.extend(torch.tensor(y))\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "        test_time.extend(timetest)\n",
    "\n",
    "    x_test = torch.tensor(x_test).reshape(-1, 1)\n",
    "    y_test = torch.tensor(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(-1, 1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "\n",
    "    x_train = torch.tensor(x_train).reshape(x_train.shape[0], 1).to(torch.float32)\n",
    "    y_train = (\n",
    "        torch.tensor(y_train)\n",
    "        .reshape(\n",
    "            -1,\n",
    "        )\n",
    "        .to(torch.float32)\n",
    "    )\n",
    "    x_train_timestamp = (\n",
    "        torch.tensor(x_train_timestamp)\n",
    "        .reshape(x_train_timestamp.shape[0], 1)\n",
    "        .to(torch.float32)\n",
    "    )\n",
    "    x_test = torch.tensor(x_test).reshape(x_test.shape[0], 1).to(torch.float32)\n",
    "    y_test = (\n",
    "        torch.tensor(y_test)\n",
    "        .reshape(\n",
    "            -1,\n",
    "        )\n",
    "        .to(torch.float32)\n",
    "    )\n",
    "    x_test_timestamp = (\n",
    "        torch.tensor(x_test_timestamp)\n",
    "        .reshape(x_test_timestamp.shape[0], 1)\n",
    "        .to(torch.float32)\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        x_train_timestamp,\n",
    "        x_test_timestamp,\n",
    "        scaler_x,\n",
    "        scaler_y,\n",
    "        scaler_time,\n",
    "        test_time,\n",
    "        train_time,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {\n",
    "    1: {\"start_time\": \"2011-04-28\", \"end_time\": \"2011-05-15\"},\n",
    "    3: {\"start_time\": \"2011-04-19\", \"end_time\": \"2011-05-22\"},\n",
    "}\n",
    "test = {\n",
    "    2: {\"start_time\": \"2011-04-21\", \"end_time\": \"2011-05-21\"},\n",
    "}\n",
    "\n",
    "appliances = [\"Microwave\", \"Refrigerator\", \"Dish Washer\"]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = False  ## To create an artificial data with added bias\n",
    "(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    x_train_timestamp,\n",
    "    x_test_timestamp,\n",
    "    scaler_x,\n",
    "    scaler_y,\n",
    "    scaler_time,\n",
    "    test_time,\n",
    "    train_time,\n",
    ") = dataset_load(appliances, train, test, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_train_timestamp.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full = x_train\n",
    "y_train = y_train\n",
    "x_test_full = x_test\n",
    "x_train_full.shape, x_test_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_model(train, test):\n",
    "    kernel = ScaleKernel(MaternKernel(nu=2.5))\n",
    "    inducing_points = x_train_full[np.arange(0, x_train_full.shape[0], 20)]\n",
    "    model = SGPRegressor(\n",
    "        x_train_full.to(\"cuda\"), y_train.to(\"cuda\"), kernel, inducing_points\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    if train:\n",
    "        loss = model.fit(lr=1e-3, n_epochs=3000, verbose=1, thetas=None, random_state=0)\n",
    "        plt.plot(np.asarray(loss[0]))\n",
    "\n",
    "        ## Save model\n",
    "        model_name = \"Point_to_point_main_power.pt\"\n",
    "        torch.save(model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "    if test:\n",
    "        model_name = \"Point_to_point_main_power.pt\"\n",
    "        model.load_state_dict(torch.load(os.path.join(\"./models\", model_name)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GP_model(train=False, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = model.predict((x_test_full).to(\"cuda\"))\n",
    "y_mean = pred_dist.loc\n",
    "y_mean = scaler_y.inverse_transform(y_mean.reshape(-1, 1).cpu()).squeeze()\n",
    "\n",
    "print(y_test.shape, y_mean.shape)\n",
    "y_mean = np.clip(y_mean, 0, y_mean.max(), out=y_mean)\n",
    "var_pred = pred_dist.variance\n",
    "var_pred = scaler_y.inverse_transform(var_pred.reshape(-1, 1).detach().cpu()).squeeze()\n",
    "std_pred = pred_dist.stddev\n",
    "std_pred = torch.tensor(\n",
    "    scaler_y.inverse_transform(std_pred.reshape(-1, 1).detach().cpu()).squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = errors.mae(torch.tensor(y_mean), y_test)\n",
    "msll = errors.msll(var_pred, y_mean, y_test)\n",
    "qce = errors.qce(std_pred, y_mean, y_test)\n",
    "print(\"mae, msll, qce - \", mae, msll, qce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [4000, 4800, 13000]\n",
    "idx = [500, 200, 300]\n",
    "\n",
    "if bias:\n",
    "    start = [4170]\n",
    "    idx = [300]\n",
    "\n",
    "x = scaler_x.inverse_transform(x_test_full.cpu().reshape(-1, 1))\n",
    "\n",
    "for i in range(len(start)):\n",
    "    if bias:\n",
    "        plot.prediction_plots(\n",
    "            x, y_test, y_mean, start[i], idx[i], var_pred, \"Point_to_point_bias\", i\n",
    "        )\n",
    "    else:\n",
    "        plot.prediction_plots(\n",
    "            x,\n",
    "            y_test,\n",
    "            y_mean,\n",
    "            start[i],\n",
    "            idx[i],\n",
    "            var_pred,\n",
    "            \"Point_to_point_plt\" + str(i + 1),\n",
    "            i,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "latexify(width_scale_factor=2.5, fig_height=1.75)\n",
    "sigma_pred = jnp.sqrt(var_pred)\n",
    "df, df1 = plot.calibration_regression(\n",
    "    y_mean.squeeze(), sigma_pred.squeeze(), y_test.squeeze(), \"test\", \"r\", ax\n",
    ")\n",
    "ax.legend()\n",
    "if bias:\n",
    "    savefig(\"Point_to_point_bias_calibration\")\n",
    "else:\n",
    "    savefig(\"Point_to_point_calibration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = errors.find_p_hat(np.array(y_test), y_mean, sigma_pred)\n",
    "p = cal.index\n",
    "mae_cal = errors.ace(p.values, cal.values)\n",
    "print(\"calibration error: \", mae_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin_max = 3000\n",
    "x_lin = np.linspace(0, x_lin_max, 15656)\n",
    "x_time = np.linspace(\n",
    "    scaler_time.inverse_transform(x_test_timestamp).min(),\n",
    "    scaler_time.inverse_transform(x_test_timestamp).max(),\n",
    "    15656,\n",
    ")\n",
    "x_lin_scale = scaler_x.transform(x_lin.reshape(-1, 1)).flatten()\n",
    "x_new = torch.tensor(x_lin_scale).reshape(-1, 1).to(torch.float32)\n",
    "\n",
    "x_new.shape, x_new.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = model.predict(x_new.to(\"cuda\"))\n",
    "y_mean = pred_dist.loc\n",
    "y_mean = scaler_y.inverse_transform(y_mean.cpu().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "latexify(width_scale_factor=2, fig_height=1.5)\n",
    "start = 500\n",
    "idx = 4000\n",
    "plt.plot(x_lin, y_mean, \"k\", label=\" Predicted Mean\", alpha=0.7)\n",
    "plt.scatter(\n",
    "    scaler_x.inverse_transform(x_train_full[:, 0].reshape(-1, 1)),\n",
    "    scaler_y.inverse_transform(y_train.reshape(-1, 1)),\n",
    "    s=4,\n",
    "    label=\"Appliance Power\",\n",
    ")\n",
    "plt.xlim(00, 1500)\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Train Mains\")\n",
    "plt.ylabel(\"Train Appliance Power\")\n",
    "plt.axvline(x=145, color=\"olive\", linestyle=\"dotted\", label=\"Mains = ~150\")\n",
    "plt.axvline(x=188, color=\"red\", linestyle=\"dotted\", label=\"Mains = ~188\")\n",
    "\n",
    "plt.axvline(x=490, color=\"magenta\", linestyle=\"dotted\")\n",
    "plt.axvline(x=1250, color=\"brown\", linestyle=\"dotted\")\n",
    "\n",
    "plt.legend(frameon=False, fontsize=6, bbox_to_anchor=(0.35, 0.55))\n",
    "savefig(\"Main_vs_app_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "latexify(width_scale_factor=2.5, fig_height=1.75)\n",
    "sns.kdeplot(\n",
    "    data={\n",
    "        \"Train Appliance\": scaler_y.inverse_transform(\n",
    "            y_train.reshape(-1, 1).cpu()\n",
    "        ).squeeze(),\n",
    "        \"Test Appliance\": (y_test.cpu()).squeeze(),\n",
    "    }\n",
    ")\n",
    "sns.despine()\n",
    "savefig(\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = test_time\n",
    "\n",
    "x_ticks_labels = pd.to_datetime(values)\n",
    "x_ticks_labels\n",
    "time_ = [(i.split(\"-04:00\")[0].strip()) for i in test_time[:]]\n",
    "\n",
    "date = [(i.split(\" \")[0].strip()) for i in time_[:]]\n",
    "mins_data = [(i.split(\" \")[1].strip()) for i in time_[:]]\n",
    "secs = [(i.split(\":00\")[1].strip()) for i in time_[:]]\n",
    "\n",
    "\n",
    "def date_con(input_string: str):\n",
    "    year, month, day = input_string.split(\"-\")\n",
    "    ret_month = \"\"\n",
    "    if int(month) == 4:\n",
    "        ret_month = \"April\"\n",
    "    elif int(month) == 5:\n",
    "        ret_month = \"May\"\n",
    "\n",
    "    ret_string = f\"{day} {ret_month}\"\n",
    "    return ret_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "start = 13100\n",
    "time_plot = scaler_time.inverse_transform(x_test_timestamp.cpu().reshape(-1, 1))\n",
    "latexify(width_scale_factor=3, fig_height=1.75)\n",
    "ax.scatter(\n",
    "    time_plot[start : start + idx],\n",
    "    scaler_x.inverse_transform(x_train)[start : start + idx],\n",
    "    label=\"Train Main\",\n",
    "    s=6,\n",
    ")\n",
    "mins = mins_data[start : start + idx]\n",
    "dates = date[start : start + idx]\n",
    "ax.set_ylabel(\"Train Mains Power\")\n",
    "plt.tick_params(\n",
    "    axis=\"x\",\n",
    "    which=\"both\",\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    labelbottom=False,\n",
    ")\n",
    "ax.set_xlabel(\n",
    "    \"Time\"\n",
    "    + \"\\n\"\n",
    "    + date_con(dates[0])\n",
    "    + \" (\"\n",
    "    + mins[0][:-3]\n",
    "    + \") \"\n",
    "    + \"to  \"\n",
    "    + date_con(dates[-1])\n",
    "    + \" (\"\n",
    "    + mins[-1][:-3]\n",
    "    + \")\"\n",
    ")\n",
    "sns.despine()\n",
    "savefig(\"Train_Scatter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a19952a8cb0d513e360355f3718fc7b5b0ccef7313ddd97e7b7ab66b1ecfbb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
